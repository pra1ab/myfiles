{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pymOhrzPxwGh",
        "outputId": "2a5cd8b9-162f-4ac2-ea30-dbcb44edefa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "• To - Do - 1:\n",
        "1. Read and Observe the Dataset.\n",
        "2. Print top(5) and bottom(5) of the dataset {Hint: pd.head and pd.tail}.\n",
        "3. Print the Information of Datasets. {Hint: pd.info}.\n",
        "4. Gather the Descriptive info about the Dataset. {Hint: pd.describe}\n",
        "5. Split your data into Feature (X) and Label (Y)."
      ],
      "metadata": {
        "id": "yXbLjLXmyqQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data =pd.read_csv(\"/content/drive/MyDrive/DATASET/student.csv\")\n",
        "\n",
        "print(\"Top 5 rows of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "print(\"\\nBottom 5 rows of the dataset:\")\n",
        "print(data.tail())\n",
        "\n",
        "print(\"\\nDataset Info:\")\n",
        "print(data.info())\n",
        "\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "print(data.describe())\n",
        "\n",
        "# Split the dataset into Features (X) and Labels (Y)\n",
        "X = data[['Math', 'Reading']]\n",
        "Y = data['Writing']\n",
        "\n",
        "# Print the feature and label data\n",
        "print(\"\\nFeature Data (X):\")\n",
        "print(X.head())\n",
        "\n",
        "print(\"\\nLabel Data (Y):\")\n",
        "print(Y.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNakMUhYyrcA",
        "outputId": "81ff4785-5d86-435b-81b3-14aa69766336"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 rows of the dataset:\n",
            "   Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n",
            "\n",
            "Bottom 5 rows of the dataset:\n",
            "     Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Math     1000 non-null   int64\n",
            " 1   Reading  1000 non-null   int64\n",
            " 2   Writing  1000 non-null   int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 23.6 KB\n",
            "None\n",
            "\n",
            "Descriptive Statistics:\n",
            "              Math      Reading      Writing\n",
            "count  1000.000000  1000.000000  1000.000000\n",
            "mean     67.290000    69.872000    68.616000\n",
            "std      15.085008    14.657027    15.241287\n",
            "min      13.000000    19.000000    14.000000\n",
            "25%      58.000000    60.750000    58.000000\n",
            "50%      68.000000    70.000000    69.500000\n",
            "75%      78.000000    81.000000    79.000000\n",
            "max     100.000000   100.000000   100.000000\n",
            "\n",
            "Feature Data (X):\n",
            "   Math  Reading\n",
            "0    48       68\n",
            "1    62       81\n",
            "2    79       80\n",
            "3    76       83\n",
            "4    59       64\n",
            "\n",
            "Label Data (Y):\n",
            "0    63\n",
            "1    72\n",
            "2    78\n",
            "3    79\n",
            "4    62\n",
            "Name: Writing, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "• To - Do - 2:"
      ],
      "metadata": {
        "id": "e05eZMZM1RQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/DATASET/student.csv\")\n",
        "\n",
        "X = data[['Math', 'Reading']].values  # Feature Matrix\n",
        "Y = data['Writing'].values\n",
        "\n",
        "X_transposed = X.T\n",
        "\n",
        "print(\"Feature Matrix (X):\")\n",
        "print(X)\n",
        "print(\"\\nTransposed Feature Matrix (X.T):\")\n",
        "print(X_transposed)\n",
        "print(\"\\nTarget Vector (Y):\")\n",
        "print(Y)\n",
        "\n",
        "W = np.random.rand(X.shape[1])\n",
        "\n",
        "print(\"\\nWeight Matrix (W):\")\n",
        "print(W)\n",
        "\n",
        "Y_pred = np.dot(W, X_transposed)\n",
        "\n",
        "print(\"\\nPredicted Values (Y_pred):\")\n",
        "print(Y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-K853GJ2qrh",
        "outputId": "97349e96-8eb3-46a8-9141-386159a431ef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Matrix (X):\n",
            "[[48 68]\n",
            " [62 81]\n",
            " [79 80]\n",
            " ...\n",
            " [89 87]\n",
            " [83 82]\n",
            " [66 66]]\n",
            "\n",
            "Transposed Feature Matrix (X.T):\n",
            "[[48 62 79 ... 89 83 66]\n",
            " [68 81 80 ... 87 82 66]]\n",
            "\n",
            "Target Vector (Y):\n",
            "[ 63  72  78  79  62  85  83  41  80  77  64  90  45  77  70  46  76  44\n",
            "  85  72  53  66  75  49  84  83  68  66  77  78  74  83  72  65  46  66\n",
            "  50  79  68  46  86  70  61  53  72  75  50  77 100  81 100  87  78  48\n",
            "  50  44  48  43  67  78  58  91  92  78  42  85  73  83  61  58  60  55\n",
            "  48  62  68  59  62  48  74  63  80  79  73  79  45  67  89  77  81  88\n",
            "  53  68  79  77  63  73  60  67 100  79  26  51  80  57  41  78  68  49\n",
            "  76  41  71  77  89  86  55  80  56  74  85  80  73  74  86  56  53  44\n",
            "  41  59  71  81  74  78  67  53  56  75  82  79  99  76  59  96  75  61\n",
            "  56  88  65 100  79  55  61  83  74  59  54  47  82  74  59  74  84  59\n",
            "  43  65  61  78  84  73  73  92  63  72  61  59  70  87  78  65  73  62\n",
            "  69  55  73  63  67  86  78  85  83  80  60  90  56  70  55  80  82  60\n",
            "  78  76  94  75  68  71  85  46  58  46  84  58  57  59  77  63  68  99\n",
            "  48  91  57  80  46  75  59  87  82  79  66  68  66  61  66  63  72  73\n",
            "  77  84  83  42  72  76  76  39  74  43  63  74  52  31  65  45  87  63\n",
            "  51  82  86  76  27  70  79  66  61  62  47  17  65  76  75  66  59  61\n",
            "  93  40  66  43  71  64  55  86  65  70  65  53  49  67  76  95  76  48\n",
            "  60  53  69  78  62  66  51  52  46  42  77  57 100  84  68  48  72  50\n",
            "  72  55  72  77  56  94  67  82  75  80  60  73  74  62  53  69  75  60\n",
            "  58  71  87  74  87  73  78  76  74  55  94  71  76  59  91  57  83  59\n",
            "  93  64  58  79  96  76  64  70  80  33  95  64  92  34  72  81  57  79\n",
            "  84  82  54  45  54  62  49  74  59  63  83  62  72  72  65  65  54  78\n",
            "  82  85  74  83  71  83  77  66  75  52  68  84  67  70  41  91  46  58\n",
            "  67  70  83  64 100  49  77  57  67  80  74  41  67  59  86  88  57  80\n",
            "  58  52  31  84  97  71  62  58  71  41  66 100  51  35  81  94  72  38\n",
            "  82  79  55  75  90  95  65  39  85  86  54  93  69  84  78  58  73  60\n",
            "  44  67  69  55  59  88  42  78  84  68  66  51  43  38  69  90  73  67\n",
            "  57  81  63  80  78  65  74  80  60  60  63  64  72  51  71  63  82  76\n",
            "  39  79  48  70  90  73  58 100  80  75  72  79  52  56  65  45  59  61\n",
            "  47  62  83  90  76  72  69  57  56  40  79  48  57  47  78  45  74  69\n",
            "  59  85  45  54  72  74  75  55  49  53  83  22 100  67  83  46  43  74\n",
            "  64  35  67  87  77  91  74  96  82  78  73  52  91  66  67  71  74  71\n",
            "  61  47  76  85  93  41  81  86  53  91  68  96  48  71  75  72  71  62\n",
            "  67  53  74  63  82  57  69  52  91  73  73  75  36  71  62 100  50  74\n",
            "  60  75  83  83 100  67  71  77  67  95  52  71  74  60  67  79  75  95\n",
            "  69  80  48  61  82  39  70  70  69  32  79  53  59  83 100  80  80  82\n",
            "  56  83  85  88  81  95  63  70  89  59  56  62  95  63  82  69  58  74\n",
            "  66  82  94  70  78  63  91  70  62  79  65  74  56  65 100  70  66  54\n",
            "  72  90  56  65  50  95  38  76  84  76  55  85  70  73  80  83  53  67\n",
            " 100  67  44  96  48  77 100  40  91  55  41  25  63  59  63  77  46  49\n",
            "  46  93  39  58  87  57  77 100  65  34  87  81  63  69  74  70  93  63\n",
            "  81  81  63  87  76  54  89  63  76  79  75  50  36  82  83  85  82  41\n",
            "  82  45  57  88  81  98  61  95  84  71  52  71  90  75  62  63  86  70\n",
            "  77  68  80  67  67  89  60  79  80  78  70  72  43  14  54  92  71  65\n",
            "  58  56  67  64  81  55  45  86  52  75  81  62  42  21  72  55  66  69\n",
            "  86  67  78  85  66  47 100  63  62  61  69  57  76  52  47  51  61  45\n",
            "  59  81  65  53  61  90  74  62  67  50  84  70  52  92  65  65  67  72\n",
            "  66  62  99  62  53  57  78  56  87  79  63  87  86  75  70  60  49  41\n",
            "  78  58  75  89  34  60  80  85  73  58  69  74  52  58  79  86  61  68\n",
            "  67  48  65  73  57  73  57  80  85  81  61  69 100  99  92  72  57  44\n",
            "  59  62  93  64  57  72  40  85  60  83  63  74  44  61  74  68  78  50\n",
            "  70  68  82  46  96 100  44  41  95  79  67  52  87  75  61  42  60  57\n",
            "  64  52  68  58  93  75  77  66  63  90  43  65  95  86  31  95  52  63\n",
            "  87  70  59  84  79  77  75  66  69  85  63  50  58  80  47  55  61  87\n",
            "  77  54  66  68  54  69  74  81  72  61  76  63  64  73  62  92  69  70\n",
            "  65  53  74  61  80  85  62  80  83  56  76  52  51  74  57  63  61  87\n",
            "  60  54  89  67  56  70  90  94  78  72]\n",
            "\n",
            "Weight Matrix (W):\n",
            "[0.43337024 0.61858353]\n",
            "\n",
            "Predicted Values (Y_pred):\n",
            "[ 62.86545119  76.97422036  83.72293089  84.27857075  65.15818971\n",
            "  81.86356261  82.29693285  49.6270402   74.68509955  85.51935551\n",
            "  69.55121805  85.63800743  51.48640849  83.35250432  69.73643134\n",
            "  48.45281682  69.92164463  49.19366996  86.44180424  78.89653231\n",
            "  64.48028023  65.52861628  73.57381982  59.46866836  82.66735942\n",
            "  85.14169352  67.69546748  70.66249778  83.78587456  78.45954436\n",
            "  78.40021841  85.57506376  73.51087616  75.6184014   51.7975091\n",
            "  58.10199627  54.45343878  80.13369936  75.67772735  45.41922514\n",
            "  85.45279414  81.43381008  66.52124409  52.41247492  68.49926429\n",
            "  74.4405603   54.88680902  73.51087616 100.86167403  70.91065473\n",
            " 105.19537641  86.68996119  76.97422036  51.04942054  51.29757749\n",
            "  53.2792154   60.8910487   52.16431797  76.05177164  80.44118227\n",
            "  60.02430822  92.44604391 103.95820936  82.5450898   53.46804639\n",
            "  94.61289511  66.3324131   92.94597553  66.58418776  64.48028023\n",
            "  67.20277128  62.06527208  52.22726163  64.97297642  66.21014348\n",
            "  52.71995783  68.314051    42.07453286  82.98207775  70.11047562\n",
            "  86.26020866  77.84096084  69.24011744  78.40021841  44.489541\n",
            "  69.4918921   85.0194239   80.50412593  79.20401522  83.22661699\n",
            "  54.94975269  75.9258843   70.54022815  78.40383612  65.96198652\n",
            "  66.70283967  59.58732027  66.70645738  98.26145259  90.83845029\n",
            "  33.47730717  54.88680902  76.48152417  48.50852506  42.75967776\n",
            "  80.50412593  81.93012398  47.58607634  80.31891265  45.29333781\n",
            "  68.43632063  80.19302532  93.06824515  88.79386872  48.32692949\n",
            "  81.43019237  55.87581912  76.9148944   92.63487491  82.36349422\n",
            "  71.4699123   83.72293089  84.0304138   75.31091849  48.07877253\n",
            "  50.49378068  37.55923489  57.60930008  79.14830697  92.07923505\n",
            "  80.38185631  74.80736917  69.79937501  58.47604055  60.45767846\n",
            "  83.28956065  89.60490095  87.62326304  89.04202567  90.96433762\n",
            "  64.66187581  97.20949883  73.38860654  71.28469901  65.71382957\n",
            "  80.74866518  77.22961273 100.86167403  82.30055055  58.7205798\n",
            "  66.76940104  86.81584852  73.38860654  62.12821575  64.60254985\n",
            "  48.76029972  87.74191495  74.81098688  60.14296013  75.9258843\n",
            "  82.97846004  63.91740495  42.94489104  71.46629459  74.93687421\n",
            "  77.65936526  94.8017261   74.37761664  67.56958015 102.10245879\n",
            "  59.46505065  76.48152417  70.79200281  60.95037465  71.34402497\n",
            "  89.84582248  88.98269971  67.82135481  74.68509955  63.1765518\n",
            "  72.4625401   62.31342904  71.84395658  66.27308714  65.40272895\n",
            "  81.67834932  78.15206145  80.25235127  88.24184657  91.58292115\n",
            "  61.8800588   98.39095763  67.82135481  78.90015002  51.79027369\n",
            "  76.72968112  93.25345844  71.53285596  74.31467297  81.49675374\n",
            "  93.00530148  72.52186606  65.15818971  76.1147153   90.90139395\n",
            "  43.31169991  63.79875304  49.56409654  77.41120831  62.06165438\n",
            "  63.42832647  67.69908519  82.36349422  54.5757084   69.61416172\n",
            " 102.10245879  45.78965172  89.78287882  63.48765242  89.6642269\n",
            "  59.22051141  76.54446783  70.17341929  83.35250432  80.3782386\n",
            "  78.77064498  65.03230238  75.49251407  64.47666252  82.17828093\n",
            "  53.0903844   65.65450362  82.54870751  73.44793249  79.57444179\n",
            "  86.93811814  84.5267277   40.46693923  70.78838511  85.88978209\n",
            "  74.56282992  38.18143613  69.24011744  53.09400211  65.343403\n",
            "  80.19302532  52.66063187  33.4143635   66.27308714  42.81900371\n",
            "  94.98332168  66.95461433  52.28658759  82.48214613  84.58605366\n",
            "  79.82259874  38.79640194  73.44793249  77.41120831  67.57319786\n",
            "  67.51025419  67.3250409   57.61291778  19.98712152  67.3250409\n",
            "  93.56455905  75.05552612  69.48827439  77.04078173  61.07264428\n",
            " 102.72104231  49.19366996  70.4809022   41.95226324  85.02304161\n",
            "  63.98396633  62.06527208  85.88978209  76.1147153   70.23274525\n",
            "  74.68871725  49.80863578  51.48279078  70.79200281  76.72968112\n",
            "  88.48638581  76.97783807  53.65325968  52.77928378  62.55796828\n",
            "  75.49613178  79.26695888  70.29568891  74.31829068  58.10561398\n",
            "  57.67224374  50.86420725  46.78227953  82.0523936   63.3653828\n",
            "  96.71680264  94.86105206  67.13982762  49.68998387  70.23274525\n",
            "  65.40996437  71.78101291  48.6380301   81.98944994  88.54932947\n",
            "  57.36114312  90.27919272  59.83547723  87.43443205  79.26695888\n",
            "  77.59642159  75.24435711  80.31891265  67.8177371   70.54384586\n",
            "  55.93876279  69.92526234  74.07013373  76.91851211  70.35863258\n",
            "  78.64837536  89.29380033  68.55859025  86.01566942  87.00467952\n",
            "  83.16367332  73.20339325  66.64351371  54.5757084  101.85430183\n",
            "  61.81349742  74.68871725  68.06951176  92.87941415  56.7425596\n",
            "  86.32315232  61.38374489  99.37996773  66.88805296  58.10199627\n",
            "  84.90077199 100.30603417  70.78838511  68.19178138  81.86718032\n",
            "  73.7554154   36.94426908  94.23885083  69.30667881  94.24246853\n",
            "  35.33305774  79.5780595   85.94910804  62.25048537  77.47053426\n",
            "  83.78225685  91.46065152  58.53898422  49.19728767  64.04690999\n",
            "  68.93625224  49.6270402   78.71131902  70.4809022   73.4515502\n",
            "  91.95334772  72.02916987  71.28469901  81.86718032  74.93687421\n",
            "  59.58732027  65.343403    75.74067102  85.14531123  93.13118881\n",
            "  90.28281043  90.03827119  71.65512558  81.49313603  78.5890494\n",
            "  71.41058634  72.89229263  55.19429193  70.60317182  87.93074595\n",
            "  64.16917961  73.01456226  48.20465986  86.87879218  59.59093798\n",
            "  56.49440265  81.0633835   73.44793249  85.63800743  69.18079148\n",
            " 101.7284145   65.77677324  76.29631088  62.99133852  72.89591034\n",
            "  83.90814418  76.29631088  53.7755293   65.89904286  65.65450362\n",
            "  95.47963558 100.24670821  66.828727    82.42282018  69.988206\n",
            "  61.75778918  42.69673409  83.72293089  98.38733993  75.9258843\n",
            "  61.1949139   61.00970061  81.43381008  52.90878882  71.90690024\n",
            " 100.05787721  49.68636616  40.5298829   77.96323046  93.00530148\n",
            "  80.07437341  43.19304799  83.10796508  86.57130928  54.76092169\n",
            "  74.99620016  88.91975605  94.23885083  74.07013373  42.75967776\n",
            "  88.6715991   86.44542195  59.9020386   98.20212664  72.64413568\n",
            "  83.28956065  89.10858704  55.81287546  76.54808554  64.66549352\n",
            "  57.30181717  72.4625401   76.17404126  62.25048537  48.38987315\n",
            "  85.94910804  46.6563922   89.97170981  74.12945968  77.91113992\n",
            "  64.97297642  47.08614473  55.69422354  53.34215906  71.22537305\n",
            "  80.93387846  71.65512558  82.11533727  66.21014348  89.29018262\n",
            "  73.76265082  82.36349422  78.8929146   70.05114967  80.19664303\n",
            "  83.34888661  71.22537305  61.20214932  66.08787385  55.44244888\n",
            "  85.64162513  61.63190185  75.98882797  74.07013373  74.87031283\n",
            "  83.35250432  50.06041044  82.42282018  55.38312293  78.5890494\n",
            "  84.34151442  71.34402497  62.74679927  98.50960955  90.53096738\n",
            "  73.07388821  72.39959644  85.82322071  55.81649316  57.42408679\n",
            "  71.03654206  41.95588094  62.93201256  63.36176509  60.14657784\n",
            "  71.16242939  73.82197678  87.12333143  80.50412593  76.54446783\n",
            "  77.6000393   59.59093798  56.92777289  47.39724534  82.73392079\n",
            "  42.88194738  66.58418776  46.40823524  86.01205171  58.35377093\n",
            "  84.5267277   69.98458829  66.64713142  80.63001327  50.98647687\n",
            "  65.53223399  80.5670696   82.17828093  75.37024444  58.97235446\n",
            "  53.7755293   62.18754171  77.03354632  31.18818635 103.02852522\n",
            "  70.16980158  86.25659095  56.7425596   49.00845668  87.12694914\n",
            "  65.96560423  45.23401186  69.36962248  87.43443205  82.73392079\n",
            "  86.13070362  87.31578013  96.77612859  85.14531123  81.49313603\n",
            "  74.19240335  57.97972665  96.09821911  66.95099662  71.71806925\n",
            "  65.22113338  78.83358865  78.40383612  68.0065681   49.07140034\n",
            "  71.65512558  83.35250432  98.57617092  46.84522319  82.0523936\n",
            "  84.89715428  62.99495623  90.34575409  68.0065681   94.79449069\n",
            "  52.03843064  68.12883772  75.74428873  76.35925454  76.9148944\n",
            "  65.46567262  65.46567262  60.20952151  71.28469901  74.81460458\n",
            "  85.38985047  64.16917961  73.94786411  57.79451336  85.94910804\n",
            "  74.93687421  67.75479343  79.51511584  45.48216881  71.09948572\n",
            "  63.05428218 105.19537641  60.58356579  74.01080777  63.858079\n",
            "  64.91003276  86.6935789   94.24246853 103.95820936  72.71069706\n",
            "  71.7773952   84.21562709  79.02241964  97.58354311  65.40634666\n",
            "  70.10685792  70.23274525  64.29144923  65.52861628  75.6184014\n",
            "  82.23760689  97.58354311  75.24435711  89.73078828  56.2498634\n",
            "  77.1037254   85.39346818  40.47055694  73.75903311  69.92164463\n",
            "  72.9552363   37.12948236  77.22237731  57.05366022  70.35501487\n",
            "  81.80423665 100.2430905   84.15630113  82.5450898   83.28594294\n",
            "  51.35690345  85.57868147  85.94910804  95.97233178  94.18314258\n",
            " 103.33962584  73.88492044  78.77426269  87.24921876  68.0065681\n",
            "  55.7535495   66.95461433  99.43929369  63.36176509  90.28642814\n",
            "  70.60317182  70.72905915  77.65574755  71.4699123   86.50836561\n",
            "  90.52734967  74.93687421  90.59391105  61.8800588   97.02428554\n",
            "  82.54870751  57.42046908  87.25283647  67.63614152  81.30792275\n",
            "  66.39897447  64.91003276 102.16178474  73.20339325  78.89653231\n",
            "  49.81225349  65.09162834  95.91300582  63.8616967   64.29144923\n",
            "  58.47965826  99.56156331  41.0261968   90.28281043  82.23760689\n",
            "  63.67286571  62.68385561  76.6074115   69.3030611   79.51511584\n",
            "  91.27543824  84.0304138   55.7535495   69.42533072 101.29504427\n",
            "  74.75166092  53.15332807  96.22410644  50.30856739  78.70770131\n",
            " 105.19537641  41.518893    97.9575874   54.01645084  49.63065791\n",
            "  28.59158263  65.343403    59.03168041  65.77677324  79.14107155\n",
            "  54.7645394   48.88256935  40.46693923  96.34637606  38.79640194\n",
            "  61.75417147  83.71931318  61.56895818  79.88554241 103.09146889\n",
            "  65.59155995  37.8667178   96.22410644  84.89715428  60.7022177\n",
            "  74.19240335  82.91913408  76.23698492  85.88616438  69.86231867\n",
            "  82.73392079  94.36835586  62.8690689   86.01205171  89.167913\n",
            "  60.26884746  88.66798139  65.77677324  74.49988626  74.87393054\n",
            "  77.03716403  60.45767846  44.80064162  86.19364729  86.13432133\n",
            "  91.6422471   74.49988626  41.08914047  75.36662674  59.83909493\n",
            "  68.25472505  83.47477394  86.63063523 102.16178474  66.27308714\n",
            " 103.33962584  88.24184657  74.38123435  65.84333461  82.17828093\n",
            "  94.79810839  70.10685792  74.81460458  63.1765518   87.80847633\n",
            "  76.35925454  74.81098688  70.85132877  87.12694914  70.48451991\n",
            "  71.10310343  88.42344214  54.14233817  74.06651602  75.30368307\n",
            "  75.11846978  71.53285596  73.88130273  53.40148502  17.38690009\n",
            "  61.56895818  94.36473816  79.5780595   72.58480973  67.63614152\n",
            "  59.7095899   74.81098688  67.20277128  83.35250432  76.17404126\n",
            "  51.42346482  90.34213639  62.00232842  81.31154046  83.10072966\n",
            "  70.05114967  48.76029972  30.62892878  81.31154046  59.09462408\n",
            "  68.37699467  78.77426269  94.36835586  68.93263453  84.77488466\n",
            "  91.02728129  68.00295039  54.94975269 105.19537641  68.0065681\n",
            "  59.33916332  67.63614152  79.70394683  57.54635641  84.34151442\n",
            "  53.83485526  56.67961593  60.76516137  70.79200281  50.06041044\n",
            "  73.38860654  88.05301557  79.70032912  52.53112683  67.5138719\n",
            "  92.44966162  81.98944994  63.55059609  67.56958015  57.17592984\n",
            "  96.65747668  71.96260849  66.15081752  99.19113674  65.58794224\n",
            "  63.54697838  77.47415197  83.53771761  67.20277128  56.43145898\n",
            " 105.19537641  56.80188555  56.06103241  71.71806925  89.47901362\n",
            "  67.94724214  88.73454276  82.85619042  64.47666252  94.8017261\n",
            "  80.3782386   72.39597873  69.36600477  79.14468926  52.59768821\n",
            "  47.52313267  72.89229263  53.83485526  83.78587456  93.25345844\n",
            "  38.17781842  65.53223399  89.72717057  96.09821911  76.54085012\n",
            "  67.3250409   72.58480973  83.28956065  58.53898422  66.58418776\n",
            "  74.74804321  86.32315232  69.05852186  69.36600477  72.15143949\n",
            "  43.18943029  76.67035516  66.64351371  58.84646713  81.0633835\n",
            "  63.3653828   80.63001327  86.07499537  73.9442464   62.25048537\n",
            "  76.79262478 102.59515498  94.79810839  87.99007191  70.2920712\n",
            "  65.40634666  51.73456544  60.52062213  64.78414543  88.73454276\n",
            "  64.47666252  69.61416172  78.71131902  49.25661363  92.50898758\n",
            "  65.34702071  87.56031938  70.78838511  75.18503116  45.54149477\n",
            "  67.51025419  70.91427244  70.78838511  82.35987651  62.80974294\n",
            "  79.14468926  73.69608945  88.54932947  55.38312293  97.70581273\n",
            " 101.79135817  47.40086305  45.91192134  93.93136792  87.06762318\n",
            "  77.53709564  57.97972665  86.75290485  83.78587456  70.9772161\n",
            "  46.04142638  64.97297642  66.08787385  71.53285596  48.81962568\n",
            "  69.24373515  61.6912278   86.38247828  81.37086641  77.96323046\n",
            "  76.04815393  58.59831017  86.38247828  56.80912097  67.01394029\n",
            "  94.98332168  82.17466322  40.22239999  98.88365383  54.20889954\n",
            "  75.86294064  86.07499537  81.12632717  65.96560423  84.0304138\n",
            "  87.56031938  72.08849582  74.00719006  73.32928058  66.45468272\n",
            "  87.12333143  63.11722585  49.31955729  61.75778918  86.75290485\n",
            "  46.10075234  59.09462408  60.32817342  94.79810839  90.03465348\n",
            "  63.48765242  70.79200281  67.01394029  60.64289175  78.58543169\n",
            "  81.86718032  84.71194099  76.67035516  69.42894843  72.77002301\n",
            "  69.61416172  71.78101291  74.87393054  64.66187581  92.6312572\n",
            "  70.16980158  79.64100317  79.01880193  54.0793945   71.40696863\n",
            "  64.91003276  84.77488466  83.90814418  72.21438315  88.61227314\n",
            "  85.5157378   52.90517112  75.30730078  64.8507068   67.63614152\n",
            "  82.79686446  68.99557819  63.42470876  67.3250409   93.31278439\n",
            "  59.03168041  59.96498226  93.99069387  67.3250409   57.30181717\n",
            "  76.97783807  84.83421061  92.38671796  86.6935789   69.42894843]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "• To - Do - 3:\n",
        "1. Split the dataset into training and test sets.\n",
        "2. You can use an 80-20 or 70-30 split, with 80% (or 70%) of the data used for training and the rest\n",
        "for testing."
      ],
      "metadata": {
        "id": "iieS-QM63sad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/DATASET/student.csv\")\n",
        "\n",
        "X = data[['Math', 'Reading']]\n",
        "Y = data['Writing']\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Shape of X_train (Features for Training):\", X_train.shape)\n",
        "print(\"Shape of X_test (Features for Testing):\", X_test.shape)\n",
        "print(\"Shape of Y_train (Target for Training):\", Y_train.shape)\n",
        "print(\"Shape of Y_test (Target for Testing):\", Y_test.shape)\n",
        "\n",
        "print(\"\\nFirst 5 rows of Training Features (X_train):\")\n",
        "print(X_train[:5])\n",
        "\n",
        "print(\"\\nFirst 5 rows of Training Targets (Y_train):\")\n",
        "print(Y_train[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cmpfL5n326Q",
        "outputId": "7a2b4d62-44d9-4a21-eea2-64415982cec2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train (Features for Training): (800, 2)\n",
            "Shape of X_test (Features for Testing): (200, 2)\n",
            "Shape of Y_train (Target for Training): (800,)\n",
            "Shape of Y_test (Target for Testing): (200,)\n",
            "\n",
            "First 5 rows of Training Features (X_train):\n",
            "     Math  Reading\n",
            "29     64       82\n",
            "535    62       70\n",
            "695    36       21\n",
            "557    81       70\n",
            "836    82       86\n",
            "\n",
            "First 5 rows of Training Targets (Y_train):\n",
            "29     78\n",
            "535    67\n",
            "695    25\n",
            "557    71\n",
            "836    87\n",
            "Name: Writing, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step -2- Build a Cost Function:"
      ],
      "metadata": {
        "id": "aPTrG7CT6WVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cost_function(X, Y, W):\n",
        "    \"\"\"\n",
        "    Calculate the Mean Squared Error (MSE) for linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    X: np.ndarray - Feature matrix.\n",
        "    Y: np.ndarray - Target vector.\n",
        "    W: np.ndarray - Weight vector.\n",
        "\n",
        "    Returns:\n",
        "    float - The cost (MSE).\n",
        "    \"\"\"\n",
        "    n = len(Y)  # Number of samples\n",
        "    Y_pred = np.dot(X, W)  # Predicted values\n",
        "    cost = (1 / (2 * n)) * np.sum((Y_pred - Y) ** 2)  # MSE formula\n",
        "    return cost\n",
        "\n",
        "# Test the cost function with sample data\n",
        "X_test = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "Y_test = np.array([3, 7, 11])\n",
        "W_test = np.array([1, 1])\n",
        "\n",
        "# Calculate the cost\n",
        "cost = cost_function(X_test, Y_test, W_test)\n",
        "print(\"Cost:\", cost)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qPDd4Sl6XNI",
        "outputId": "dafb6480-f9c9-4447-9a9d-f09ac14d1a96"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step -3- Gradient Descent for Simple Linear Regression:\n"
      ],
      "metadata": {
        "id": "7lDCmzoc62HO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix (m x n).\n",
        "    Y (numpy.ndarray): Target vector (m x 1).\n",
        "    W (numpy.ndarray): Initial guess for parameters (n x 1).\n",
        "    alpha (float): Learning rate.\n",
        "    iterations (int): Number of iterations for gradient descent.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the final optimized parameters and the history of cost values.\n",
        "    \"\"\"\n",
        "    m = len(Y)  # number of samples\n",
        "    cost_history = []\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        Y_pred = X.dot(W)  # predicted values\n",
        "        loss = Y_pred - Y  # difference between predicted and actual\n",
        "        gradient = (1 / m) * X.T.dot(loss)  # gradient calculation\n",
        "        W -= alpha * gradient  # update weights\n",
        "        cost = cost_function(X, Y, W)  # calculate cost\n",
        "        cost_history.append(cost)  # store cost history\n",
        "\n",
        "    return W, cost_history\n",
        "\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 2)\n",
        "Y = np.random.rand(100, 1)\n",
        "W = np.random.rand(2, 1)\n",
        "\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "\n",
        "final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
        "print(\"Final Parameters:\", final_params)\n",
        "print(\"Cost History:\", cost_history)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umz9PTcx7MU0",
        "outputId": "7ac4529d-6358-4153-b935-95bee2d24997"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Parameters: [[0.44044412]\n",
            " [0.46329667]]\n",
            "Cost History: [0.14660222125987915, 0.14559739250130407, 0.14460408845943903, 0.14362217675920552, 0.14265152654634444, 0.14169200846994356, 0.1407434946651655, 0.1398058587361739, 0.13887897573925595, 0.13796272216613892, 0.13705697592749816, 0.13616161633665466, 0.13527652409346005, 0.13440158126836665, 0.1335366712866802, 0.13268167891299418, 0.13183649023580252, 0.13100099265228907, 0.13017507485329222, 0.1293586268084416, 0.12855153975146635, 0.12775370616567144, 0.1269650197695815, 0.12618537550274903, 0.12541466951172592, 0.12465279913619645, 0.12389966289526878, 0.12315516047392504, 0.12241919270962603, 0.12169166157907058, 0.12097247018510665, 0.12026152274379294, 0.11955872457160915, 0.11886398207281311, 0.11817720272694329, 0.11749829507646484, 0.11682716871455737, 0.11616373427304348, 0.1155079034104555, 0.1148595888002397, 0.11421870411909585, 0.11358516403545055, 0.11295888419806367, 0.11233978122476483, 0.11172777269131982, 0.11112277712042498, 0.11052471397082758, 0.10993350362657157, 0.10934906738636704, 0.10877132745308114, 0.10820020692335057, 0.10763562977731266, 0.10707752086845504, 0.10652580591358154, 0.10598041148289364, 0.10544126499018558, 0.1049082946831523, 0.10438142963380885, 0.10386059972901965, 0.10334573566113651, 0.10283676891874467, 0.10233363177751448, 0.10183625729115889, 0.10134457928249414, 0.10085853233460385, 0.10037805178210384, 0.09990307370250825, 0.09943353490769409, 0.09896937293546397, 0.09851052604120619, 0.09805693318965032, 0.09760853404671739, 0.097165268971464, 0.09672707900811896, 0.09629390587821111, 0.09586569197278795, 0.09544238034472347, 0.09502391470111449, 0.09461023939576406, 0.09420129942175173, 0.09379704040408869, 0.09339740859245753, 0.09300235085403542, 0.09261181466639976, 0.092225748110515, 0.0918440998638008, 0.09146681919327876, 0.09109385594879908, 0.09072516055634396, 0.09036068401140872, 0.09000037787245838, 0.08964419425445995, 0.08929208582248876, 0.08894400578540808, 0.08859990788962163, 0.08825974641289788, 0.08792347615826511, 0.08759105244797662, 0.08726243111754553, 0.08693756850984784, 0.0866164214692933, 0.08629894733606344, 0.08598510394041556, 0.08567484959705218, 0.08536814309955547, 0.08506494371488511, 0.08476521117794, 0.08446890568618189, 0.08417598789432104, 0.08388641890906293, 0.08360016028391534, 0.08331717401405501, 0.08303742253125329, 0.08276086869886017, 0.08248747580684583, 0.08221720756689928, 0.08195002810758326, 0.08168590196954463, 0.08142479410078025, 0.08116666985195663, 0.08091149497178388, 0.08065923560244255, 0.0804098582750631, 0.08016332990525722, 0.07991961778870058, 0.07967868959676624, 0.0794405133722084, 0.07920505752489551, 0.07897229082759263, 0.07874218241179197, 0.07851470176359165, 0.07828981871962165, 0.07806750346301641, 0.07784772651943413, 0.07763045875312125, 0.07741567136302266, 0.07720333587893627, 0.07699342415771196, 0.07678590837949394, 0.07658076104400663, 0.07637795496688292, 0.07617746327603474, 0.07597925940806538, 0.07578331710472291, 0.07558961040939444, 0.0753981136636405, 0.07520880150376959, 0.07502164885745156, 0.07483663094037048, 0.07465372325291546, 0.07447290157690988, 0.07429414197237778, 0.07411742077434799, 0.07394271458969427, 0.07377000029401246, 0.07359925502853285, 0.0734304561970686, 0.07326358146299879, 0.07309860874628644, 0.07293551622053053, 0.072774282310052, 0.07261488568701346, 0.07245730526857147, 0.0723015202140621, 0.0721475099222184, 0.07199525402842012, 0.07184473240197478, 0.07169592514343029, 0.07154881258191811, 0.0714033752725272, 0.07125959399370795, 0.07111744974470605, 0.07097692374302589, 0.07083799742192293, 0.07070065242792499, 0.07056487061838201, 0.07043063405904408, 0.07029792502166692, 0.0701667259816453, 0.07003701961567353, 0.0699087887994326, 0.06978201660530416, 0.06965668630011065, 0.06953278134288145, 0.06941028538264454, 0.06928918225624386, 0.06916945598618134, 0.069051090778484, 0.06893407102059547, 0.06881838127929171, 0.0687040062986208, 0.06859093099786619, 0.06847914046953349, 0.06836861997736042, 0.06825935495434955, 0.06815133100082364, 0.06804453388250335, 0.06793894952860703, 0.06783456402997237, 0.06773136363719952, 0.06762933475881579, 0.06752846395946105, 0.06742873795809438, 0.0673301436262211, 0.06723266798614008, 0.0671362982092116, 0.06704102161414456, 0.06694682566530372, 0.06685369797103642, 0.06676162628201825, 0.06667059848961798, 0.06658060262428123, 0.06649162685393249, 0.0664036594823959, 0.06631668894783377, 0.06623070382120325, 0.06614569280473083, 0.06606164473040405, 0.06597854855848084, 0.06589639337601585, 0.06581516839540372, 0.06573486295293908, 0.06565546650739319, 0.06557696863860675, 0.06549935904609902, 0.06542262754769296, 0.06534676407815598, 0.06527175868785651, 0.06519760154143578, 0.06512428291649514, 0.06505179320229823, 0.06498012289848813, 0.06490926261381932, 0.06483920306490412, 0.06476993507497354, 0.06470144957265239, 0.06463373759074857, 0.06456679026505595, 0.06450059883317141, 0.06443515463332522, 0.06437044910322483, 0.06430647377891216, 0.0642432202936338, 0.06418068037672428, 0.06411884585250224, 0.06405770863917912, 0.0639972607477805, 0.06393749428107988, 0.06387840143254453, 0.06381997448529361, 0.06376220581106833, 0.06370508786921353, 0.06364861320567157, 0.06359277445198729, 0.0635375643243246, 0.0634829756224944, 0.0634290012289935, 0.0633756341080548, 0.06332286730470828, 0.06327069394385265, 0.06321910722933795, 0.06316810044305837, 0.06311766694405586, 0.06306780016763358, 0.06301849362448005, 0.06296974089980281, 0.06292153565247263, 0.06287387161417703, 0.06282674258858377, 0.06278014245051408, 0.06273406514512497, 0.06268850468710144, 0.06264345515985756, 0.06259891071474696, 0.06255486557028238, 0.06251131401136403, 0.06246825038851693, 0.06242566911713707, 0.062383564676746106, 0.06234193161025473, 0.06230076452323429, 0.062260058083197185, 0.06221980701888503, 0.06218000611956543, 0.06214065023433652, 0.06210173427143973, 0.06206325319758026, 0.062025202037255464, 0.061987575872091, 0.061950369840184386, 0.061913579135456365, 0.06187719900700966, 0.06184122475849482, 0.061805651747483835, 0.06177047538485045, 0.06173569113415802, 0.06170129451105414, 0.06166728108267223, 0.061633646467040205, 0.061600386332495705, 0.06156749639710814, 0.061534972428107466, 0.06150281024131928, 0.061471005700606714, 0.06143955471731853, 0.06140845324974349, 0.06137769730257134, 0.06134728292635954, 0.06131720621700654, 0.061287463315230724, 0.061258050406055704, 0.06122896371830119, 0.061200199524079964, 0.06117175413830057, 0.06114362391817555, 0.06111580526273559, 0.061088294612349076, 0.06106108844824725, 0.061034183292054706, 0.06100757570532547, 0.060981262289084244, 0.06095523968337297, 0.06092950456680264, 0.06090405365611026, 0.060878883705720775, 0.06085399150731422, 0.06082937388939768, 0.06080502771688233, 0.06078094989066509, 0.0607571373472154, 0.0607335870581665, 0.06071029602991142, 0.06068726130320381, 0.060664479952763195, 0.06064194908688465, 0.060619665847053306, 0.060597627407562936, 0.06057583097513914, 0.06055427378856673, 0.06053295311832157, 0.06051186626620635, 0.060491010564990906, 0.060470383378056325, 0.06044998209904345, 0.0604298041515052, 0.060409846988562985, 0.060390108092566966, 0.06037058497476045, 0.060351275174947885, 0.06033217626116673, 0.06031328582936329, 0.060294601503071955, 0.06027612093309844, 0.060257841797206356, 0.0602397617998077, 0.06022187867165668, 0.06020419016954704, 0.060186694076013084, 0.0601693881990339, 0.060152270371741076, 0.06013533845212989, 0.060118590322773474, 0.06010202389054076, 0.060085637086317166, 0.06006942786472891, 0.06005339420387006, 0.060037534105033305, 0.060021845592443175, 0.06000632671299272, 0.05999097553598319, 0.059975790152866554, 0.059960768676991176, 0.05994590924335008, 0.059931210008332576, 0.0599166691494783, 0.059902284865234384, 0.059888055374715234, 0.05987397891746518, 0.05986005375322374, 0.05984627816169372, 0.0598326504423119, 0.05981916891402227, 0.05980583191505209, 0.059792637802690285, 0.05977958495306858, 0.059766671760945016, 0.059753896639489996, 0.05974125802007481, 0.059728754352062596, 0.059716384102601654, 0.05970414575642117, 0.05969203781562924, 0.059680058799513366, 0.05966820724434297, 0.05965648170317448, 0.05964488074565844, 0.0596334029578489, 0.05962204694201507, 0.05961081131645501, 0.05959969471531154, 0.05958869578839027, 0.05957781320097965, 0.05956704563367322, 0.05955639178219374, 0.05954585035721943, 0.059535420084212226, 0.059525099703248, 0.05951488796884859, 0.05950478364981606, 0.05949478552906849, 0.05948489240347803, 0.05947510308371045, 0.05946541639406684, 0.05945583117232699, 0.05944634626959453, 0.059436960550143914, 0.059427672891269136, 0.05941848218313435, 0.05940938732862573, 0.05940038724320569, 0.059391480854768225, 0.05938266710349627, 0.059373944941720415, 0.05936531333377954, 0.059356771255882776, 0.05934831769597326, 0.059339951653593355, 0.059331672139751324, 0.059323478176789846, 0.05931536879825566, 0.05930734304877103, 0.05929939998390637, 0.05929153867005474, 0.05928375818430732, 0.059276057614330684, 0.05926843605824535, 0.05926089262450563, 0.05925342643178103, 0.059246036608838946, 0.059238722294428586, 0.059231482637166474, 0.05922431679542298, 0.059217223937210475, 0.05921020324007253, 0.0592032538909743, 0.059196375086194644, 0.05918956603121888, 0.05918282594063318, 0.059176154038020075, 0.05916954955585511, 0.059163011735404655, 0.059156539826625075, 0.0591501330880629, 0.05914379078675607, 0.059137512198136566, 0.05913129660593397, 0.0591251433020801, 0.059119051586614936, 0.05911302076759346, 0.05910705016099362, 0.0591011390906253, 0.059095286888040485, 0.059089492892444265, 0.05908375645060698, 0.05907807691677741, 0.0590724536525968, 0.05906688602701404, 0.0590613734162017, 0.059055915203473155, 0.0590505107792005, 0.0590451595407336, 0.05903986089231984, 0.05903461424502506, 0.05902941901665515, 0.05902427463167873, 0.059019180521150585, 0.05901413612263611, 0.05900914088013649, 0.05900419424401486, 0.05899929567092317, 0.058994444623730116, 0.05898964057144962, 0.05898488298917033, 0.05898017135798587, 0.05897550516492591, 0.05897088390288797, 0.05896630707057009, 0.05896177417240419, 0.05895728471849025, 0.05895283822453128, 0.05894843421176889, 0.05894407220691982, 0.058939751742112946, 0.058935472354827295, 0.05893123358783047, 0.05892703498911805, 0.05892287611185346, 0.058918756514308786, 0.05891467575980598, 0.05891063341665895, 0.05890662905811623, 0.0589026622623044, 0.05889873261217193, 0.05889483969543401, 0.0588909831045177, 0.05888716243650786, 0.058883377293093675, 0.058879627280515885, 0.05887591200951442, 0.05887223109527676, 0.05886858415738699, 0.058864970819775195, 0.058861390710667615, 0.05885784346253739, 0.05885432871205579, 0.05885084610004385, 0.058847395271425025, 0.05884397587517791, 0.05884058756428967, 0.05883722999571017, 0.05883390283030639, 0.05883060573281742, 0.0588273383718102, 0.058824100419635324, 0.05882089155238381, 0.05881771144984411, 0.05881455979545965, 0.05881143627628687, 0.058808340582953804, 0.05880527240961907, 0.05880223145393125, 0.05879921741698902, 0.05879623000330143, 0.058793268920748744, 0.05879033388054383, 0.05878742459719383, 0.058784540788462424, 0.05878168217533242, 0.05877884848196881, 0.05877603943568226, 0.05877325476689297, 0.05877049420909505, 0.05876775749882123, 0.058765044375607936, 0.05876235458196089, 0.05875968786332098, 0.05875704396803059, 0.05875442264730034, 0.05875182365517612, 0.058749246748506644, 0.05874669168691113, 0.05874415823274764, 0.058741646151081656, 0.05873915520965494, 0.058736685178854844, 0.05873423583168399, 0.058731806943730216, 0.05872939829313698, 0.05872700966057395, 0.058724640829208107, 0.05872229158467498, 0.05871996171505044, 0.05871765101082265, 0.0587153592648644, 0.0587130862724057, 0.05871083183100683, 0.058708595740531465, 0.058706377803120456, 0.05870417782316548, 0.05870199560728338, 0.058699830964290545, 0.05869768370517774, 0.05869555364308515, 0.05869344059327765, 0.05869134437312054, 0.05868926480205541, 0.058687201701576285, 0.05868515489520613, 0.058683124208473506, 0.05868110946888971, 0.05867911050592586, 0.058677127150990556, 0.05867515923740758, 0.0586732066003939, 0.05867126907703806, 0.05866934650627865, 0.05866743872888309, 0.05866554558742663, 0.058663666926271694, 0.058661802591547296, 0.05865995243112889, 0.05865811629461819, 0.05865629403332353, 0.05865448550024015, 0.05865269055003099, 0.05865090903900746, 0.05864914082511057, 0.05864738576789231, 0.05864564372849698, 0.05864391456964326, 0.05864219815560582, 0.05864049435219767, 0.05863880302675249, 0.05863712404810716, 0.05863545728658459, 0.05863380261397661, 0.05863215990352716, 0.05863052902991564, 0.058628909869240464, 0.05862730229900283, 0.05862570619809051, 0.0586241214467621, 0.05862254792663123, 0.058620985520651044, 0.05861943411309883, 0.05861789358956092, 0.05861636383691755, 0.05861484474332817, 0.05861333619821666, 0.05861183809225695, 0.0586103503173586, 0.05860887276665269, 0.058607405334477836, 0.05860594791636627, 0.05860450040903025, 0.058603062710348486, 0.05860163471935281, 0.05860021633621492, 0.05859880746223335, 0.05859740799982055, 0.05859601785249012, 0.05859463692484419, 0.05859326512256096, 0.05859190235238236, 0.058590548522101826, 0.05858920354055238, 0.05858786731759457, 0.05858653976410476, 0.05858522079196354, 0.05858391031404413, 0.05858260824420106, 0.058581314497258986, 0.05858002898900146, 0.058578751636160026, 0.05857748235640335, 0.05857622106832643, 0.05857496769144012, 0.05857372214616047, 0.05857248435379847, 0.05857125423654984, 0.05857003171748476, 0.058568816720537965, 0.05856760917049881, 0.05856640899300146, 0.058565216114515296, 0.05856403046233528, 0.058562851964572456, 0.05856168055014479, 0.05856051614876773, 0.05855935869094517, 0.05855820810796047, 0.05855706433186743, 0.058555927295481515, 0.05855479693237121, 0.05855367317684922, 0.05855255596396416, 0.058551445229492016, 0.05855034090992782, 0.05854924294247746, 0.05854815126504953, 0.05854706581624725, 0.05854598653536063, 0.05854491336235846, 0.05854384623788066, 0.058542785103230556, 0.058541729900367295, 0.05854068057189832, 0.058539637061072, 0.05853859931177026, 0.05853756726850138, 0.05853654087639273, 0.05853552008118383, 0.0585345048292192, 0.0585334950674416, 0.058532490743385034, 0.05853149180516809, 0.05853049820148721, 0.058529509881610126, 0.05852852679536921, 0.05852754889315526, 0.05852657612591078, 0.05852560844512397, 0.05852464580282235, 0.0585236881515666, 0.05852273544444447, 0.05852178763506477, 0.058520844677551434, 0.05851990652653759, 0.05851897313715975, 0.058518044465052085, 0.05851712046634068, 0.05851620109763802, 0.058515286316037286, 0.058514376079107006, 0.05851347034488551, 0.05851256907187561, 0.05851167221903936, 0.058510779745792656, 0.0585098916120002, 0.0585090077779703, 0.0585081282044498, 0.05850725285261912, 0.058506381684087234, 0.05850551466088688, 0.05850465174546957, 0.058503792900700964, 0.05850293808985604, 0.05850208727661448, 0.058501240425055986, 0.0585003974996558, 0.0584995584652801, 0.0584987232871816, 0.05849789193099509, 0.058497064362733164, 0.05849624054878179, 0.05849542045589612, 0.058494604051196275, 0.05849379130216318, 0.05849298217663438, 0.05849217664280012, 0.05849137466919919, 0.05849057622471497, 0.05848978127857156, 0.058488989800329814, 0.05848820175988361, 0.058487417127455914, 0.05848663587359511, 0.05848585796917133, 0.058485083385372685, 0.05848431209370172, 0.058483544065971754, 0.05848277927430342, 0.05848201769112115, 0.058481259289149636, 0.0584805040414105, 0.05847975192121885, 0.058479002902179986, 0.05847825695818602, 0.05847751406341274, 0.058476774192316186, 0.05847603731962965, 0.05847530342036041, 0.058474572469786636, 0.058473844443454305, 0.058473119317174166, 0.0584723970670187, 0.05847167766931916, 0.0584709611006626, 0.058470247337889004, 0.05846953635808839, 0.05846882813859791, 0.058468122656999195, 0.05846741989111539, 0.058466719819008466, 0.0584660224189766, 0.05846532766955134, 0.05846463554949506, 0.05846394603779829, 0.05846325911367712, 0.058462574756570684, 0.05846189294613852, 0.05846121366225817, 0.05846053688502268, 0.058459862594738086, 0.058459190771921084, 0.05845852139729656, 0.0584578544517953, 0.058457189916551584, 0.0584565277729009, 0.05845586800237766, 0.05845521058671295, 0.058454555507832275, 0.05845390274785339, 0.05845325228908408, 0.058452604114020014, 0.05845195820534262, 0.058451314545916934, 0.05845067311878962, 0.058450033907186796, 0.05844939689451209, 0.05844876206434453, 0.05844812940043665, 0.05844749888671248, 0.058446870507265575, 0.058446244246357144, 0.05844562008841411, 0.05844499801802729, 0.05844437801994944, 0.05844376007909352, 0.058443144180530815, 0.05844253030948916, 0.05844191845135123, 0.05844130859165263, 0.05844070071608032, 0.05844009481047085, 0.05843949086080861, 0.05843888885322428, 0.05843828877399302, 0.05843769060953299, 0.058437094346403595, 0.058436499971304, 0.058435907471071476, 0.05843531683267988, 0.05843472804323807, 0.058434141089988414, 0.0584335559603053, 0.058432972641693566, 0.058432391121787115, 0.05843181138834739, 0.05843123342926197, 0.05843065723254315, 0.05843008278632651, 0.05842951007886954, 0.058428939098550216, 0.05842836983386574, 0.058427802273431126, 0.05842723640597784, 0.058426672220352575, 0.058426109705515865, 0.05842554885054087, 0.058424989644612076, 0.05842443207702404, 0.058423876137180156, 0.058423321814591374, 0.05842276909887513, 0.05842221797975397, 0.05842166844705455, 0.05842112049070626, 0.05842057410074023, 0.05842002926728815, 0.058419485980581075, 0.058418944230948365, 0.0584184040088166, 0.0584178653047084, 0.058417328109241405, 0.05841679241312725, 0.0584162582071704, 0.0584157254822672, 0.058415194229404825, 0.058414664439660195, 0.05841413610419904, 0.0584136092142749, 0.0584130837612281, 0.058412559736484786, 0.05841203713155601, 0.05841151593803667, 0.05841099614760478, 0.058410477752020226, 0.05840996074312413, 0.0584094451128378, 0.05840893085316191, 0.05840841795617546, 0.058407906414035135, 0.0584073962189742, 0.05840688736330181, 0.058406379839402056, 0.05840587363973323, 0.058405368756826864, 0.05840486518328704, 0.05840436291178948, 0.058403861935080834, 0.05840336224597782, 0.05840286383736644, 0.058402366702201274, 0.05840187083350466, 0.05840137622436592, 0.05840088286794066, 0.05840039075745005, 0.05839989988618001, 0.05839941024748053, 0.058398921834765014, 0.0583984346415095, 0.05839794866125193, 0.058397463887591615, 0.058396980314188385, 0.058396497934761975, 0.05839601674309142, 0.058395536733014286, 0.05839505789842615, 0.05839458023327982, 0.058394103731584775, 0.05839362838740656, 0.05839315419486606, 0.058392681148139024, 0.058392209241455335, 0.05839173846909849, 0.05839126882540498, 0.05839080030476367, 0.058390332901615286, 0.05838986661045177, 0.058389401425815786, 0.058388937342300065, 0.05838847435454694, 0.05838801245724778, 0.058387551645142394, 0.058387091913018586, 0.05838663325571151, 0.05838617566810326, 0.05838571914512228, 0.058385263681742856, 0.0583848092729847, 0.05838435591391229, 0.05838390359963455, 0.05838345232530421, 0.05838300208611745, 0.058382552877313296, 0.05838210469417326, 0.05838165753202084, 0.058381211386220995, 0.05838076625217981, 0.058380322125343914, 0.05837987900120014, 0.05837943687527501, 0.05837899574313436, 0.05837855560038289, 0.05837811644266366, 0.058377678265657824, 0.05837724106508407, 0.058376804836698284, 0.058376369576293156, 0.05837593527969769, 0.0583755019427769, 0.05837506956143142, 0.05837463813159697, 0.058374207649244225, 0.058373778110378145, 0.05837334951103786, 0.05837292184729611, 0.05837249511525899, 0.05837206931106558, 0.05837164443088746, 0.05837122047092854, 0.05837079742742462, 0.05837037529664296, 0.05836995407488216, 0.05836953375847154, 0.05836911434377108, 0.058368695827170865, 0.0583682782050909, 0.05836786147398074, 0.05836744563031914, 0.058367030670613794, 0.058366616591401015, 0.05836620338924536, 0.05836579106073942, 0.05836537960250343, 0.05836496901118508, 0.05836455928345907, 0.058364150416026946, 0.05836374240561677, 0.05836333524898281, 0.05836292894290531, 0.058362523484190146, 0.058362118869668626, 0.05836171509619712, 0.05836131216065689, 0.0583609100599538, 0.05836050879101798, 0.05836010835080369, 0.058359708736288915, 0.05835930994447524, 0.05835891197238755, 0.05835851481707376, 0.05835811847560459, 0.05835772294507332, 0.05835732822259559, 0.05835693430530904, 0.05835654119037326, 0.05835614887496938, 0.05835575735629993, 0.05835536663158865, 0.058354976698080166, 0.05835458755303984, 0.058354199193753495, 0.05835381161752728, 0.05835342482168738, 0.05835303880357985, 0.058352653560570386, 0.058352269090044095, 0.058351885389405334, 0.058351502456077516, 0.05835112028750281, 0.05835073888114208, 0.05835035823447464, 0.058349978344997985, 0.05834959921022769, 0.05834922082769718, 0.05834884319495759, 0.05834846630957751, 0.05834809016914286]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Evaluate the Model\n",
        "Implementing RMSE Function\n",
        "Implementing R² Function"
      ],
      "metadata": {
        "id": "M1AI0-Ol79AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the Root Mean Square Error (RMSE).\n",
        "\n",
        "    Parameters:\n",
        "    Y (numpy.ndarray): Actual target values.\n",
        "    Y_pred (numpy.ndarray): Predicted target values.\n",
        "\n",
        "    Returns:\n",
        "    float: The RMSE value.\n",
        "    \"\"\"\n",
        "    return np.sqrt(np.mean((Y - Y_pred) ** 2))\n",
        "\n",
        "def r2(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the R-squared value.\n",
        "\n",
        "    Parameters:\n",
        "    Y (numpy.ndarray): Actual target values.\n",
        "    Y_pred (numpy.ndarray): Predicted target values.\n",
        "\n",
        "    Returns:\n",
        "    float: The R-squared value.\n",
        "    \"\"\"\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)  # Sum of squared residuals\n",
        "    ss_tot = np.sum((Y - np.mean(Y)) ** 2)  # Total sum of squares\n",
        "    return 1 - (ss_res / ss_tot)\n",
        "\n",
        "print(\"RMSE:\", rmse(Y, Y_pred))\n",
        "print(\"R²:\", r2(Y, Y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ7q4bGl8iL4",
        "outputId": "4909b302-b150-4402-ad5e-98d0b7ba83c3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 73.39084313513128\n",
            "R²: -59181847.32829826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step -5- Main Function to Integrate All Steps:"
      ],
      "metadata": {
        "id": "ORNjViNQ9Hwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def main():\n",
        "    # Step 1: Load the dataset\n",
        "    data = pd.read_csv(\"/content/drive/MyDrive/DATASET/student.csv\")\n",
        "\n",
        "def rmse(Y, Y_pred):\n",
        "    \"\"\"Calculate the Root Mean Square Error (RMSE).\"\"\"\n",
        "    return np.sqrt(np.mean((Y - Y_pred) ** 2))\n",
        "\n",
        "def r2(Y, Y_pred):\n",
        "    \"\"\"Calculate the R-squared value.\"\"\"\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)  # Sum of squared residuals\n",
        "    ss_tot = np.sum((Y - np.mean(Y)) ** 2)  # Total sum of squares\n",
        "    return 1 - (ss_res / ss_tot)\n",
        "\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"Perform gradient descent to learn weights.\"\"\"\n",
        "    n = len(Y)\n",
        "    cost_history = []\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        Y_pred = X.dot(W)\n",
        "        cost = (1 / (2 * n)) * np.sum((Y_pred - Y) ** 2)  # Mean Squared Error\n",
        "        cost_history.append(cost)\n",
        "\n",
        "        gradient = (1 / n) * X.T.dot(Y_pred - Y)  # Gradient\n",
        "        W -= alpha * gradient  # Update weights\n",
        "\n",
        "    return W, cost_history\n",
        "\n",
        "def main():\n",
        "    # Load the dataset\n",
        "    data = pd.read_csv(\"/content/drive/MyDrive/DATASET/student.csv\")\n",
        "\n",
        "    # Split the data into features (X) and target (Y)\n",
        "    X = data[['Math', 'Reading']].values\n",
        "    Y = data['Writing'].values.reshape(-1, 1)\n",
        "\n",
        "    # Check for NaN values in the dataset\n",
        "    if np.any(np.isnan(X)) or np.any(np.isnan(Y)):\n",
        "        print(\"Data contains NaN values. Please clean the dataset.\")\n",
        "        return\n",
        "\n",
        "    # Normalize the features\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "    # Add bias term to feature matrix\n",
        "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "\n",
        "    # Initialize weights with small random values\n",
        "    W = np.random.rand(X.shape[1], 1) * 0.01\n",
        "\n",
        "    # Set hyperparameters\n",
        "    alpha = 0.001  # Reduced learning rate\n",
        "    iterations = 1000\n",
        "\n",
        "    # Train the model using gradient descent\n",
        "    final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
        "\n",
        "    # Make predictions\n",
        "    Y_pred = X.dot(final_params)\n",
        "\n",
        "    # Evaluate the model\n",
        "    model_rmse = rmse(Y, Y_pred)\n",
        "    model_r2 = r2(Y, Y_pred)\n",
        "\n",
        "    print(\"Final Parameters:\", final_params)\n",
        "    print(\"Cost History:\", cost_history[-5:])\n",
        "    print(\"RMSE:\", model_rmse)\n",
        "    print(\"R²:\", model_r2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuPswV159SqR",
        "outputId": "b33da0f6-7720-4cf1-f6d4-56e1ea508dd2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Parameters: [[43.38675105]\n",
            " [ 5.13466913]\n",
            " [ 7.22723725]]\n",
            "Cost History: [339.2698259097862, 338.61567645440743, 337.9628474511243, 337.31133620476504, 336.66114002575]\n",
            "RMSE: 25.923435583659806\n",
            "R²: -1.8958517353696194\n"
          ]
        }
      ]
    }
  ]
}